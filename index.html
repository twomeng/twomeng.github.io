<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <section id="main">
  
    <article id="post-hexo发布带图片博客" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/19/hexo发布带图片博客/">hexo发布带图片博客</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/09/19/hexo发布带图片博客/" class="article-date">
  <time datetime="2018-09-19T02:55:03.000Z" itemprop="datePublished">2018-09-19</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/2018/09/19/hexo发布带图片博客/5ad6be319fcbd.jpg" alt=""></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n,k;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">"%d%d"</span>,&amp;n,&amp;k)!=EOF)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (n%<span class="number">10</span> == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                n = n/<span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                n -- ;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d"</span>,n);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span> ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
a+b+c</script>
      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/14/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/09/14/hello-world/" class="article-date">
  <time datetime="2018-09-14T08:11:58.295Z" itemprop="datePublished">2018-09-14</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-效率啊效率" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/08/效率啊效率/">效率啊效率</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/09/08/效率啊效率/" class="article-date">
  <time datetime="2018-09-07T21:06:00.000Z" itemprop="datePublished">2018-09-08</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>感觉虽然自己一整天都坐在图书馆里，但读起东西来漫无边际，不像某些钻透牛角尖的人，不把问题研究清楚是真真儿不会罢休的类型。昨天一天浏览的那些网页，倒不如晚上在床上思考时读的那些东西。所以不要看着大家都坐在这里，我仿佛看到了大家时间上的流逝，而我追求的是高效的学习，认真的生活，我可以压缩学习时间，提高学习效率，适当地进行休息有利于我高效的思考活动，挤出来的时间可以读书、写作、运动。这才是我想要的。虽然每次等待面试的过程总是漫长而又艰辛，但我也要把握住点，第三个问题没有思路那么我可以把前两个问题完美地回答上来。</p>
<p>所以今天探索的主题是，如何回答问题能抓住老师的心。如何高效地学习。</p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-变得美而自信" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/07/变得美而自信/">变得美而自信</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/09/07/变得美而自信/" class="article-date">
  <time datetime="2018-09-06T20:51:00.000Z" itemprop="datePublished">2018-09-07</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>前两天龙老师悄咪咪地给我说，他想看到我大一时又白又细又美的腿，我想了想，这大概是想给我拍美美的照片，让我提前练练腿的意思？天秤男喜欢从头到脚都完美的女神，我也希望能在各个方面变得女神，因为我很明显没有达到他的要求。要身材好，用心打扮自己，穿合适的衣服，大部分时间独立自主地做自己喜欢的事情，有自己的爱好，变得更加优秀。所以你现在不应该想他了，控制欲望，做最应该做的事情。</p>
<p>送个水果受到了深深的打击。设定刘昊然小哥哥为壁纸，向他学习，做个钢铁直女，从此不撩任何汉子，再撩我他妈就是一只狗。寻找女性好友，不要再和男孩子接触了，记住，做一个钢铁直女，不受伤也不伤害到其他人。</p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-重回图书馆之日" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/05/重回图书馆之日/">重回图书馆之日</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/09/05/重回图书馆之日/" class="article-date">
  <time datetime="2018-09-04T22:40:00.000Z" itemprop="datePublished">2018-09-05</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>与他谈妥。无碍。又得重新立志准备九推。约得一同道中人，共勉与图书馆五楼。闭微信、远手机，旁置汪先生一本《独酌》，我自独酌之，伴有愁心与明月。琢磨文字，实乃人生一大乐趣。</p>
<p>九月推免，近在眼前，上周所联系的导师，两日后便又要与我相见，无论前几日多么放荡不羁爱自由，这两日也要收收心、争分夺秒地把状态调整到最好，故情爱滋味、暂且放一放，晚上月下相伴而行，便是最好的滋味了。脑中浮现他的影子，侧颜除了嘴巴稍突，均为完美轮廓，很幸福，年方21岁，头一次被人宠溺地叫宝宝，说来未免脸红心跳不止，恋爱实乃一件上瘾事，Taylor男友多达百位，估计亦是由于难逃甜蜜恋爱滋味的魔爪罢了。但厉害的女子，懂得自我管理，管理身形，不多吃、不沾油腻垃圾；管理情绪，忍小事成大器；管理思想，不任其为水中浮萍，随波飘摇，而是有根有形，绵延其上，可以理出一条结结实实的线路来。新鲜的爱情，于我是一种思想上的考验，怎样在理性和感性之间抉择，怎样在走神时迅速地把情绪抓回来，归根到底便是”摆脱欲望”四个字罢了。摆脱欲望，有利于身心健康，不矫情不做作，真真切切地爱护爱人，这便是这段注定要分手的恋爱维系下去的方法吧。</p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-coursera 机器学习 多分类+神经网络的项目" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/27/coursera 机器学习 多分类+神经网络的项目/">coursera 机器学习 多分类+神经网络的项目</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/08/27/coursera 机器学习 多分类+神经网络的项目/" class="article-date">
  <time datetime="2018-08-26T22:15:00.000Z" itemprop="datePublished">2018-08-27</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Loading and Visualizing Data</p>
<p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180827082118896-1230325417.png" alt=""></p>
<p> Vectorize Logistic Regression</p>
<p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180827091510711-137864264.png" alt=""></p>
<p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180827140855164-1647737552.png" alt=""></p>
<p> 有点懒得粘代码。。直接Po上GitHub！：</p>
<p><a href="https://github.com/twomeng/multiple_classify-and-neural-network" target="_blank" rel="noopener">https://github.com/twomeng/multiple_classify-and-neural-network</a></p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-coursera 机器学习 logistic regression 逻辑回归的项目" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/25/coursera 机器学习 logistic regression 逻辑回归的项目/">coursera 机器学习 logistic regression 逻辑回归的项目</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/08/25/coursera 机器学习 logistic regression 逻辑回归的项目/" class="article-date">
  <time datetime="2018-08-24T19:50:00.000Z" itemprop="datePublished">2018-08-25</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>github : <a href="https://github.com/twomeng/logistic-regression-" target="_blank" rel="noopener">https://github.com/twomeng/logistic-regression-</a></p>
<p>ex1. m</p>
<pre><code>  1 %% Machine Learning Online Class - Exercise 2: Logistic Regression
  2 %
  3 %  Instructions
  4 %  ------------
  5 % 
  6 %  This file contains code that helps you get started on the logistic
  7 %  regression exercise. You will need to complete the following functions 
  8 %  in this exericse:
  9 %
 10 %     sigmoid.m
 11 %     costFunction.m
 12 %     predict.m
 13 %     costFunctionReg.m
 14 %
 15 %  For this exercise, you will not need to change any code in this file,
 16 %  or any other files other than those mentioned above.
 17 %
 18 
 19 %% Initialization
 20 clear ; close all; clc
 21 
 22 %% Load Data
 23 %  The first two columns contains the exam scores and the third column
 24 %  contains the label.
 25 
 26 data = load(&#39;ex2data1.txt&#39;);
 27 X = data(:, [1, 2]); y = data(:, 3);
 28 
 29 %% ==================== Part 1: Plotting ====================
 30 %  We start the exercise by first plotting the data to understand the 
 31 %  the problem we are working with.
 32 
 33 fprintf([&#39;Plotting data with + indicating (y = 1) examples and o &#39; ...
 34          &#39;indicating (y = 0) examples.\n&#39;]);
 35 
 36 plotData(X, y);
 37 
 38 % Put some labels 
 39 hold on;
 40 % Labels and Legend
 41 xlabel(&#39;Exam 1 score&#39;)
 42 ylabel(&#39;Exam 2 score&#39;)
 43 
 44 % Specified in plot order
 45 legend(&#39;Admitted&#39;, &#39;Not admitted&#39;)
 46 hold off;
 47 
 48 fprintf(&#39;\nProgram paused. Press enter to continue.\n&#39;);
 49 pause;
 50 
 51 
 52 %% ============ Part 2: Compute Cost and Gradient ============
 53 %  In this part of the exercise, you will implement the cost and gradient
 54 %  for logistic regression. You neeed to complete the code in 
 55 %  costFunction.m
 56 
 57 %  Setup the data matrix appropriately, and add ones for the intercept term
 58 [m, n] = size(X);
 59 
 60 % Add intercept term to x and X_test
 61 X = [ones(m, 1) X];
 62 
 63 % Initialize fitting parameters
 64 initial_theta = zeros(n + 1, 1);
 65 
 66 % Compute and display initial cost and gradient
 67 [cost, grad] = costFunction(initial_theta, X, y);
 68 
 69 fprintf(&#39;Cost at initial theta (zeros): %f\n&#39;, cost);
 70 fprintf(&#39;Expected cost (approx): 0.693\n&#39;);
 71 fprintf(&#39;Gradient at initial theta (zeros): \n&#39;);
 72 fprintf(&#39; %f \n&#39;, grad);
 73 fprintf(&#39;Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n&#39;);
 74 
 75 % Compute and display cost and gradient with non-zero theta
 76 test_theta = [-24; 0.2; 0.2];
 77 [cost, grad] = costFunction(test_theta, X, y);
 78 
 79 fprintf(&#39;\nCost at test theta: %f\n&#39;, cost);
 80 fprintf(&#39;Expected cost (approx): 0.218\n&#39;);
 81 fprintf(&#39;Gradient at test theta: \n&#39;);
 82 fprintf(&#39; %f \n&#39;, grad);
 83 fprintf(&#39;Expected gradients (approx):\n 0.043\n 2.566\n 2.647\n&#39;);
 84 
 85 fprintf(&#39;\nProgram paused. Press enter to continue.\n&#39;);
 86 pause;
 87 
 88 
 89 %% ============= Part 3: Optimizing using fminunc  =============
 90 %  In this exercise, you will use a built-in function (fminunc) to find the
 91 %  optimal parameters theta.
 92 
 93 %  Set options for fminunc
 94 options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 400);
 95 
 96 %  Run fminunc to obtain the optimal theta
 97 %  This function will return theta and the cost 
 98 [theta, cost] = ...
 99     fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
100 
101 % Print theta to screen
102 fprintf(&#39;Cost at theta found by fminunc: %f\n&#39;, cost);
103 fprintf(&#39;Expected cost (approx): 0.203\n&#39;);
104 fprintf(&#39;theta: \n&#39;);
105 fprintf(&#39; %f \n&#39;, theta);
106 fprintf(&#39;Expected theta (approx):\n&#39;);
107 fprintf(&#39; -25.161\n 0.206\n 0.201\n&#39;);
108 
109 % Plot Boundary
110 plotDecisionBoundary(theta, X, y);
111 
112 % Put some labels 
113 hold on;
114 % Labels and Legend
115 xlabel(&#39;Exam 1 score&#39;)
116 ylabel(&#39;Exam 2 score&#39;)
117 
118 % Specified in plot order
119 legend(&#39;Admitted&#39;, &#39;Not admitted&#39;)
120 hold off;
121 
122 fprintf(&#39;\nProgram paused. Press enter to continue.\n&#39;);
123 pause;
124 
125 %% ============== Part 4: Predict and Accuracies ==============
126 %  After learning the parameters, you&#39;ll like to use it to predict the outcomes
127 %  on unseen data. In this part, you will use the logistic regression model
128 %  to predict the probability that a student with score 45 on exam 1 and 
129 %  score 85 on exam 2 will be admitted.
130 %
131 %  Furthermore, you will compute the training and test set accuracies of 
132 %  our model.
133 %
134 %  Your task is to complete the code in predict.m
135 
136 %  Predict probability for a student with score 45 on exam 1 
137 %  and score 85 on exam 2 
138 
139 prob = sigmoid([1 45 85] * theta);
140 fprintf([&#39;For a student with scores 45 and 85, we predict an admission &#39; ...
141          &#39;probability of %f\n&#39;], prob);
142 fprintf(&#39;Expected value: 0.775 +/- 0.002\n\n&#39;);
143 
144 % Compute accuracy on our training set
145 p = predict(theta, X);
146 
147 fprintf(&#39;Train Accuracy: %f\n&#39;, mean(double(p == y)) * 100);
148 fprintf(&#39;Expected accuracy (approx): 89.0\n&#39;);
149 fprintf(&#39;\n&#39;);
</code></pre><p>plotData. m</p>
<pre><code> 1 function plotData(X, y)
 2 %PLOTDATA Plots the data points X and y into a new figure 
 3 %   PLOTDATA(x,y) plots the data points with + for the positive examples
 4 %   and o for the negative examples. X is assumed to be a Mx2 matrix.
 5 
 6 % Create New Figure
 7 figure; hold on;
 8 
 9 % ====================== YOUR CODE HERE ======================
10 % Instructions: Plot the positive and negative examples on a
11 %               2D plot, using the option &#39;k+&#39; for the positive
12 %               examples and &#39;ko&#39; for the negative examples.
13 %
14 
15 pos = find(y==1); % find() return the position of y == 1 
16 neg = find(y==0); 
17 
18 plot(X(pos,1),X(pos,2),&#39;k+&#39;,&#39;LineWidth&#39;,2,&#39;MarkerSize&#39;,7);
19 plot(X(neg,1),X(neg,2),&#39;ko&#39;,&#39;LineWidth&#39;,2,&#39;MarkerFaceColor&#39;,&#39;y&#39;,&#39;MarkerSize&#39;,7);
20 
21 
22 
23 
24 
25 
26 
27 % =========================================================================
28 
29 
30 
31 hold off;
32 
33 end
</code></pre><p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180825101202108-1602936479.png" alt=""></p>
<p><strong>costFuction.m  </strong></p>
<pre><code> 1 function [J, grad] = costFunction(theta, X, y)
 2 %COSTFUNCTION Compute cost and gradient for logistic regression
 3 %   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
 4 %   parameter for logistic regression and the gradient of the cost
 5 %   w.r.t. to the parameters.
 6 
 7 % Initialize some useful values
 8 m = length(y); % number of training examples
 9 
10 % You need to return the following variables correctly 
11 J = 0;
12 grad = zeros(size(theta));
13 
14 % ====================== YOUR CODE HERE ======================
15 % Instructions: Compute the cost of a particular choice of theta.
16 %               You should set J to the cost.
17 %               Compute the partial derivatives and set grad to the partial
18 %               derivatives of the cost w.r.t. each parameter in theta
19 %
20 % Note: grad should have the same dimensions as theta
21 %
22 alpha = 1 ;
23 J = (-1/m) * ( y&#39; * log(sigmoid(X * theta)) + ( 1-y )&#39;* log(1-sigmoid(X*theta)));
24 grad = ( alpha / m ) * X&#39;* (sigmoid( X * theta ) - y);
25 
26 
27 
28 
29 
30 
31 % =============================================================
32 
33 end
</code></pre><p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180825104137298-1330427443.png" alt=""></p>
<p> 内置函数fminuc()</p>
<p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180825104319799-1270567464.png" alt=""></p>
<p>绘制边界函数：</p>
<pre><code>if size(X, 2) &lt;= 3
    % Only need 2 points to define a line, so choose two endpoints 选择在坐标轴上的两个临界点
    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];  

    % Calculate the decision boundary line theta(1) + theta(2) * x(1) + theta(3) * x(2) = 0 计算两个y在边界上的值
    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));

    % Plot, and adjust axes for better viewing 两点确定一条直线
    plot(plot_x, plot_y)

    % Legend, specific for the exercise
    legend(&#39;Admitted&#39;, &#39;Not admitted&#39;, &#39;Decision Boundary&#39;)
    axis([30, 100, 30, 100])
</code></pre><p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180825105437107-2008155299.png" alt=""></p>
<p>predict. m</p>
<pre><code> 1 function p = predict(theta, X)
 2 %PREDICT Predict whether the label is 0 or 1 using learned logistic 
 3 %regression parameters theta
 4 %   p = PREDICT(theta, X) computes the predictions for X using a 
 5 %   threshold at 0.5 (i.e., if sigmoid(theta&#39;*x) &gt;= 0.5, predict 1)
 6 
 7 m = size(X, 1); % Number of training examples
 8 
 9 % You need to return the following variables correctly
10 p = zeros(m, 1);
11 
12 % ====================== YOUR CODE HERE ======================
13 % Instructions: Complete the following code to make predictions using
14 %               your learned logistic regression parameters. 
15 %               You should set p to a vector of 0&#39;s and 1&#39;s
16 %
17 
18 % cal h(x) -- predictions 
19 % pos = find(sigmoid( X * theta ) &gt;= 0.5 );
20 % neg = find(sigmoid( x * theta ) &lt; 0.5 );
21 p = sigmoid( X * theta ); 
22 for i = 1:m
23     if p(i) &gt;= 0.5 
24         p(i) = 1;
25     else 
26         p(i) = 0;
27     end 
28 end 
29 % =========================================================================
30 end
</code></pre><p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180825111157263-664382418.png" alt=""></p>
<p>ex2_reg. m</p>
<pre><code>  1 %% Machine Learning Online Class - Exercise 2: Logistic Regression
  2 %
  3 %  Instructions
  4 %  ------------
  5 %
  6 %  This file contains code that helps you get started on the second part
  7 %  of the exercise which covers regularization with logistic regression.
  8 %
  9 %  You will need to complete the following functions in this exericse:
 10 %
 11 %     sigmoid.m
 12 %     costFunction.m
 13 %     predict.m
 14 %     costFunctionReg.m
 15 %
 16 %  For this exercise, you will not need to change any code in this file,
 17 %  or any other files other than those mentioned above.
 18 %
 19 
 20 %% Initialization
 21 clear ; close all; clc
 22 
 23 %% Load Data
 24 %  The first two columns contains the X values and the third column
 25 %  contains the label (y).
 26 
 27 data = load(&#39;ex2data2.txt&#39;);
 28 X = data(:, [1, 2]); y = data(:, 3);
 29 
 30 plotData(X, y);
 31 
 32 % Put some labels
 33 hold on;
 34 
 35 % Labels and Legend
 36 xlabel(&#39;Microchip Test 1&#39;)
 37 ylabel(&#39;Microchip Test 2&#39;)
 38 
 39 % Specified in plot order
 40 legend(&#39;y = 1&#39;, &#39;y = 0&#39;)
 41 hold off;
 42 
 43 
 44 %% =========== Part 1: Regularized Logistic Regression ============
 45 %  In this part, you are given a dataset with data points that are not
 46 %  linearly separable. However, you would still like to use logistic
 47 %  regression to classify the data points.
 48 %
 49 %  To do so, you introduce more features to use -- in particular, you add
 50 %  polynomial features to our data matrix (similar to polynomial
 51 %  regression).
 52 %
 53 
 54 % Add Polynomial Features
 55 
 56 % Note that mapFeature also adds a column of ones for us, so the intercept
 57 % term is handled
 58 X = mapFeature(X(:,1), X(:,2));
 59 
 60 % Initialize fitting parameters
 61 initial_theta = zeros(size(X, 2), 1);
 62 
 63 % Set regularization parameter lambda to 1
 64 lambda = 1;
 65 
 66 % Compute and display initial cost and gradient for regularized logistic
 67 % regression
 68 [cost, grad] = costFunctionReg(initial_theta, X, y, lambda);
 69 
 70 fprintf(&#39;Cost at initial theta (zeros): %f\n&#39;, cost);
 71 fprintf(&#39;Expected cost (approx): 0.693\n&#39;);
 72 fprintf(&#39;Gradient at initial theta (zeros) - first five values only:\n&#39;);
 73 fprintf(&#39; %f \n&#39;, grad(1:5));
 74 fprintf(&#39;Expected gradients (approx) - first five values only:\n&#39;);
 75 fprintf(&#39; 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115\n&#39;);
 76 
 77 fprintf(&#39;\nProgram paused. Press enter to continue.\n&#39;);
 78 pause;
 79 
 80 % Compute and display cost and gradient
 81 % with all-ones theta and lambda = 10
 82 test_theta = ones(size(X,2),1);
 83 [cost, grad] = costFunctionReg(test_theta, X, y, 10);
 84 
 85 fprintf(&#39;\nCost at test theta (with lambda = 10): %f\n&#39;, cost);
 86 fprintf(&#39;Expected cost (approx): 3.16\n&#39;);
 87 fprintf(&#39;Gradient at test theta - first five values only:\n&#39;);
 88 fprintf(&#39; %f \n&#39;, grad(1:5));
 89 fprintf(&#39;Expected gradients (approx) - first five values only:\n&#39;);
 90 fprintf(&#39; 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922\n&#39;);
 91 
 92 fprintf(&#39;\nProgram paused. Press enter to continue.\n&#39;);
 93 pause;
 94 
 95 %% ============= Part 2: Regularization and Accuracies =============
 96 %  Optional Exercise:
 97 %  In this part, you will get to try different values of lambda and
 98 %  see how regularization affects the decision coundart
 99 %
100 %  Try the following values of lambda (0, 1, 10, 100).
101 %
102 %  How does the decision boundary change when you vary lambda? How does
103 %  the training set accuracy vary?
104 %
105 
106 % Initialize fitting parameters
107 initial_theta = zeros(size(X, 2), 1);
108 
109 % Set regularization parameter lambda to 1 (you should vary this)
110 lambda = 1;
111 
112 % Set Options
113 options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, 400);
114 
115 % Optimize
116 [theta, J, exit_flag] = ...
117     fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);
118 
119 % Plot Boundary
120 plotDecisionBoundary(theta, X, y);
121 hold on;
122 title(sprintf(&#39;lambda = %g&#39;, lambda))
123 
124 % Labels and Legend
125 xlabel(&#39;Microchip Test 1&#39;)
126 ylabel(&#39;Microchip Test 2&#39;)
127 
128 legend(&#39;y = 1&#39;, &#39;y = 0&#39;, &#39;Decision boundary&#39;)
129 hold off;
130 
131 % Compute accuracy on our training set
132 p = predict(theta, X);
133 
134 fprintf(&#39;Train Accuracy: %f\n&#39;, mean(double(p == y)) * 100);
135 fprintf(&#39;Expected accuracy (with lambda = 1): 83.1 (approx)\n&#39;);
</code></pre><p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180825113540011-1894022320.png" alt=""></p>
<p>costFunction_Reg.m</p>
<pre><code> 1 function [J, grad] = costFunctionReg(theta, X, y, lambda)
 2 %COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization
 3 %   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using
 4 %   theta as the parameter for regularized logistic regression and the
 5 %   gradient of the cost w.r.t. to the parameters. 
 6 
 7 % Initialize some useful values
 8 m = length(y); % number of training examples
 9 
10 % You need to return the following variables correctly 
11 J = 0;
12 grad = zeros(size(theta));
13 
14 % ====================== YOUR CODE HERE ======================
15 % Instructions: Compute the cost of a particular choice of theta.
16 %               You should set J to the cost.
17 %               Compute the partial derivatives and set grad to the partial
18 %               derivatives of the cost w.r.t. each parameter in theta
19 alpha = 1;
20 other = lambda ./ (2 * m) * theta.^2 ; 
21 J = (-1/m) * ( y&#39; * log(sigmoid(X * theta)) + ( 1-y )&#39;* log(1-sigmoid(X*theta))) + other ;
22 grad = ( alpha / m ) * X&#39;* (sigmoid( X * theta ) - y) + lambda ./ m * theta ;
23 
24 % =============================================================
25 
26 end
</code></pre>
      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-coursera 机器学习 linear regression 线性回归的小项目" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/24/coursera 机器学习 linear regression 线性回归的小项目/">coursera 机器学习 linear regression 线性回归的小项目</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/08/24/coursera 机器学习 linear regression 线性回归的小项目/" class="article-date">
  <time datetime="2018-08-24T01:40:00.000Z" itemprop="datePublished">2018-08-24</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Matlab 环境：</p>
<p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180824174223165-1235656193.png" alt=""></p>
<p>1. 一元线性回归</p>
<p>ex1.m</p>
<pre><code>%% Machine Learning Online Class - Exercise 1: Linear Regression

%  Instructions
%  ------------
%
%  This file contains code that helps you get started on the
%  linear exercise. You will need to complete the following functions
%  in this exericse:
%
%     warmUpExercise.m
%     plotData.m  绘制房屋面积和房屋价格的点状图
%     gradientDescent.m 梯度下降法求theta 
%     computeCost.m 计算损失函数
%     gradientDescentMulti.m
%     computeCostMulti.m
%     featureNormalize.m 特征缩放
%     normalEqn.m 正规方程法求theta
%
%  For this exercise, you will not need to change any code in this file,
%  or any other files other than those mentioned above.
%
% x refers to the population size in 10,000s
% y refers to the profit in $10,000s
%

%% Initialization
clear ; close all; clc

%% ==================== Part 1: Basic Function ====================
% Complete warmUpExercise.m
fprintf(&#39;Running warmUpExercise ... \n&#39;);
fprintf(&#39;5x5 Identity Matrix: \n&#39;);
warmUpExercise()

fprintf(&#39;Program paused. Press enter to continue.\n&#39;);
pause;


%% ======================= Part 2: Plotting =======================
fprintf(&#39;Plotting Data ...\n&#39;)
data = load(&#39;ex1data1.txt&#39;);
X = data(:, 1); y = data(:, 2);
m = length(y); % number of training examples

% Plot Data
% Note: You have to complete the code in plotData.m
plotData(X, y);

fprintf(&#39;Program paused. Press enter to continue.\n&#39;);
pause;

%% =================== Part 3: Cost and Gradient descent ===================

X = [ones(m, 1), data(:,1)]; % Add a column of ones to x
theta = zeros(2, 1); % initialize fitting parameters

% Some gradient descent settings
iterations = 1500;
alpha = 0.01;

fprintf(&#39;\nTesting the cost function ...\n&#39;)
% compute and display initial cost
J = computeCost(X, y, theta);
fprintf(&#39;With theta = [0 ; 0]\nCost computed = %f\n&#39;, J);
fprintf(&#39;Expected cost value (approx) 32.07\n&#39;);

% further testing of the cost function
J = computeCost(X, y, [-1 ; 2]);
fprintf(&#39;\nWith theta = [-1 ; 2]\nCost computed = %f\n&#39;, J);
fprintf(&#39;Expected cost value (approx) 54.24\n&#39;);

fprintf(&#39;Program paused. Press enter to continue.\n&#39;);
pause;

fprintf(&#39;\nRunning Gradient Descent ...\n&#39;)
% run gradient descent
theta = gradientDescent(X, y, theta, alpha, iterations);

% print theta to screen
fprintf(&#39;Theta found by gradient descent:\n&#39;);
fprintf(&#39;%f\n&#39;, theta);
fprintf(&#39;Expected theta values (approx)\n&#39;);
fprintf(&#39; -3.6303\n  1.1664\n\n&#39;);

% Plot the linear fit
hold on; % keep previous plot visible
plot(X(:,2), X*theta, &#39;-&#39;)
legend(&#39;Training data&#39;, &#39;Linear regression&#39;)
hold off % don&#39;t overlay any more plots on this figure

% Predict values for population sizes of 35,000 and 70,000
predict1 = [1, 3.5] *theta;
fprintf(&#39;For population = 35,000, we predict a profit of %f\n&#39;,...
    predict1*10000);
predict2 = [1, 7] * theta;
fprintf(&#39;For population = 70,000, we predict a profit of %f\n&#39;,...
    predict2*10000);

fprintf(&#39;Program paused. Press enter to continue.\n&#39;);
pause;

%% ============= Part 4: Visualizing J(theta_0, theta_1) =============
fprintf(&#39;Visualizing J(theta_0, theta_1) ...\n&#39;)

% Grid over which we will calculate J
theta0_vals = linspace(-10, 10, 100);
theta1_vals = linspace(-1, 4, 100);

% initialize J_vals to a matrix of 0&#39;s
J_vals = zeros(length(theta0_vals), length(theta1_vals));

% Fill out J_vals
for i = 1:length(theta0_vals)
    for j = 1:length(theta1_vals)
      t = [theta0_vals(i); theta1_vals(j)];
      J_vals(i,j) = computeCost(X, y, t);
    end
end


% Because of the way meshgrids work in the surf command, we need to
% transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals&#39;;
% Surface plot
figure;
surf(theta0_vals, theta1_vals, J_vals)
xlabel(&#39;\theta_0&#39;); ylabel(&#39;\theta_1&#39;);

% Contour plot
figure;
% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))
xlabel(&#39;\theta_0&#39;); ylabel(&#39;\theta_1&#39;);
hold on;
plot(theta(1), theta(2), &#39;rx&#39;, &#39;MarkerSize&#39;, 10, &#39;LineWidth&#39;, 2);

![](https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180824174727529-1033549378.png)






computeCost.m   




function J = computeCost(X, y, theta)
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.

h = X * theta ; % cal the hypothesis
J = 1/(2*m) * sum((h-y) .^ 2 ) ;



% =========================================================================

end  

gradientDescent.m   





function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters

    % ====================== YOUR CODE HERE ======================
    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCost) and gradient here.
    %
h = X * theta ; 
h_minus  = h - y;
h_sum = ( h_minus&#39; * X )&#39;;
theta = theta - alpha * h_sum ./ m; 

    % ============================================================

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);

end

end  


![](https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180824174815347-167117428.png)
</code></pre><p><strong>多元线性回归</strong></p>
<p><strong>ex1_multi.m  </strong></p>
<pre><code>  1 %% Machine Learning Online Class
  2 %  Exercise 1: Linear regression with multiple variables
  3 %
  4 %  Instructions
  5 %  ------------
  6 % 
  7 %  This file contains code that helps you get started on the
  8 %  linear regression exercise. 
  9 %
 10 %  You will need to complete the following functions in this 
 11 %  exericse:
 12 %
 13 %     warmUpExercise.m
 14 %     plotData.m
 15 %     gradientDescent.m
 16 %     computeCost.m
 17 %     gradientDescentMulti.m
 18 %     computeCostMulti.m
 19 %     featureNormalize.m
 20 %     normalEqn.m
 21 %
 22 %  For this part of the exercise, you will need to change some
 23 %  parts of the code below for various experiments (e.g., changing
 24 %  learning rates).
 25 %
 26 
 27 %% Initialization
 28 
 29 %% ================ Part 1: Feature Normalization ================
 30 
 31 %% Clear and Close Figures
 32 clear ; close all; clc
 33 
 34 fprintf(&#39;Loading data ...\n&#39;);
 35 
 36 %% Load Data
 37 data = load(&#39;ex1data2.txt&#39;);
 38 X = data(:, 1:2);
 39 y = data(:, 3);
 40 m = length(y);
 41 
 42 % Print out some data points
 43 fprintf(&#39;First 10 examples from the dataset: \n&#39;);
 44 fprintf(&#39; x = [%.0f %.0f], y = %.0f \n&#39;, [X(1:10,:) y(1:10,:)]&#39;);
 45 
 46 fprintf(&#39;Program paused. Press enter to continue.\n&#39;);
 47 pause;
 48 
 49 % Scale features and set them to zero mean
 50 fprintf(&#39;Normalizing Features ...\n&#39;);
 51 
 52 [X mu sigma] = featureNormalize(X);
 53 
 54 % Add intercept term to X
 55 X = [ones(m, 1) X];
 56 
 57 
 58 %% ================ Part 2: Gradient Descent ================
 59 
 60 % ====================== YOUR CODE HERE ======================
 61 % Instructions: We have provided you with the following starter
 62 %               code that runs gradient descent with a particular
 63 %               learning rate (alpha). 
 64 %
 65 %               Your task is to first make sure that your functions - 
 66 %               computeCost and gradientDescent already work with 
 67 %               this starter code and support multiple variables.
 68 %
 69 %               After that, try running gradient descent with 
 70 %               different values of alpha and see which one gives
 71 %               you the best result.
 72 %
 73 %               Finally, you should complete the code at the end
 74 %               to predict the price of a 1650 sq-ft, 3 br house.
 75 %
 76 % Hint: By using the &#39;hold on&#39; command, you can plot multiple
 77 %       graphs on the same figure.
 78 %
 79 % Hint: At prediction, make sure you do the same feature normalization.
 80 %
 81 
 82 fprintf(&#39;Running gradient descent ...\n&#39;);
 83 
 84 % Choose some alpha value
 85 alpha = 0.01;
 86 num_iters = 400;
 87 
 88 % Init Theta and Run Gradient Descent 
 89 theta = zeros(3, 1);
 90 [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);
 91 
 92 % Plot the convergence graph
 93 figure;
 94 plot(1:numel(J_history), J_history, &#39;-b&#39;, &#39;LineWidth&#39;, 2);
 95 xlabel(&#39;Number of iterations&#39;);
 96 ylabel(&#39;Cost J&#39;);
 97 
 98 % Display gradient descent&#39;s result
 99 fprintf(&#39;Theta computed from gradient descent: \n&#39;);
100 fprintf(&#39; %f \n&#39;, theta);
101 fprintf(&#39;\n&#39;);
102 
103 % Estimate the price of a 1650 sq-ft, 3 br house
104 % ====================== YOUR CODE HERE ======================
105 % Recall that the first column of X is all-ones. Thus, it does
106 % not need to be normalized.
107 price = 0; % You should change this
108 X_1 = [1 1650 3] ; 
109 price  = X_1 * theta ;
110 % ============================================================
111 
112 fprintf([&#39;Predicted price of a 1650 sq-ft, 3 br house &#39; ...
113          &#39;(using gradient descent):\n $%f\n&#39;], price);
114 
115 fprintf(&#39;Program paused. Press enter to continue.\n&#39;);
116 pause;
117 
118 %% ================ Part 3: Normal Equations ================
119 
120 fprintf(&#39;Solving with normal equations...\n&#39;);
121 
122 % ====================== YOUR CODE HERE ======================
123 % Instructions: The following code computes the closed form 
124 %               solution for linear regression using the normal
125 %               equations. You should complete the code in 
126 %               normalEqn.m
127 %
128 %               After doing so, you should complete this code 
129 %               to predict the price of a 1650 sq-ft, 3 br house.
130 %
131 
132 %% Load Data
133 data = csvread(&#39;ex1data2.txt&#39;);
134 X = data(:, 1:2);
135 y = data(:, 3);
136 m = length(y);
137 
138 % Add intercept term to X
139 X = [ones(m, 1) X];
140 
141 % Calculate the parameters from the normal equation
142 theta = normalEqn(X, y);
143 
144 % Display normal equation&#39;s result
145 fprintf(&#39;Theta computed from the normal equations: \n&#39;);
146 fprintf(&#39; %f \n&#39;, theta);
147 fprintf(&#39;\n&#39;);
148 
149 
150 % Estimate the price of a 1650 sq-ft, 3 br house
151 % ====================== YOUR CODE HERE ======================
152 price = 0; % You should change this
153 X_2 = [ 1 1650 3] ; 
154 price  = X_2 * theta ;
155 % ============================================================
156 
157 fprintf([&#39;Predicted price of a 1650 sq-ft, 3 br house &#39; ...
158          &#39;(using normal equations):\n $%f\n&#39;], price);
</code></pre><p>特征缩放</p>
<pre><code> 1 function [X_norm, mu, sigma] = featureNormalize(X)
 2 %FEATURENORMALIZE Normalizes the features in X 
 3 %   FEATURENORMALIZE(X) returns a normalized version of X where
 4 %   the mean value of each feature is 0 and the standard deviation
 5 %   is 1. This is often a good preprocessing step to do when
 6 %   working with learning algorithms.
 7 
 8 % You need to set these values correctly
 9 X_norm = X;
10 mu = zeros(1, size(X, 2));
11 sigma = zeros(1, size(X, 2));
12 
13 % ====================== YOUR CODE HERE ======================
14 % Instructions: First, for each feature dimension, compute the mean
15 %               of the feature and subtract it from the dataset,
16 %               storing the mean value in mu. Next, compute the 
17 %               standard deviation of each feature and divide
18 %               each feature by it&#39;s standard deviation, storing
19 %               the standard deviation in sigma. 
20 %
21 %               Note that X is a matrix where each column is a 
22 %               feature and each row is an example. You need 
23 %               to perform the normalization separately for 
24 %               each feature. 
25 %
26 % Hint: You might find the &#39;mean&#39; and &#39;std&#39; functions useful.
27 %       
28 mu = mean(X); % 1 * ( n + 1 )
29 sigma = std(X);
30 for i = 1 : size(X,1)
31     X_norm(i,:) = ( X_norm(i,:) - mu ) ./ sigma ; % 对每个行向量做减法
32 end
33 %  X_norm(1:10,:)
34 
35 % ============================================================
36 
37 end
</code></pre><p>computeCostMulti and gradientDescent是没有变的。</p>
<p>正规方程法</p>
<pre><code> 1 function [theta] = normalEqn(X, y)
 2 %NORMALEQN Computes the closed-form solution to linear regression 
 3 %   NORMALEQN(X,y) computes the closed-form solution to linear 
 4 %   regression using the normal equations.
 5 
 6 theta = zeros(size(X, 2), 1);
 7 
 8 % ====================== YOUR CODE HERE ======================
 9 % Instructions: Complete the code to compute the closed form solution
10 %               to linear regression and put the result in theta.
11 %
12 
13 % ---------------------- Sample Solution ----------------------
14 
15 theta = pinv(X&#39;*X)*X&#39;*y; 
16 
17 
18 % -------------------------------------------------------------
19 
20 
21 % ============================================================
22 
23 end
</code></pre><p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180824175213120-1029827607.png" alt=""></p>
<p><img src="https://images2018.cnblogs.com/blog/1464741/201808/1464741-20180824175234755-1542726331.png" alt=""></p>
<p>源码：</p>
<p> <a href="https://github.com/twomeng/linear-regression-" target="_blank" rel="noopener">https://github.com/twomeng/linear-regression-</a></p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-每天中午日常绷不住" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/24/每天中午日常绷不住/">每天中午日常绷不住</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/08/24/每天中午日常绷不住/" class="article-date">
  <time datetime="2018-08-24T00:37:00.000Z" itemprop="datePublished">2018-08-24</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>是的上午连着学了五个小时，中午的时候一开始玩就绷不住了。玩到了下午四点唉下午的大好时光本来是能把代码搞完的现在还需要耗费新的时间来搞，我真是太不争气了唉。每次给自己安排这么多任务，我又做不完，还不是得好几天才能搞完，而且还非常打击我的自信心。哭泣，又玩了一会儿抖音，刷视频真的停不下来，而且都是假的！假的！你以为开了美颜滤镜大眼白皙的抖音里的你就是真的你了吗？！Nope<br>！<br>都是假的！假的！看看现实生活中的人，不要再沉迷于不重要的事情了，当个男孩子的好处就是不需要care那么多，而女孩子真的好麻烦！啊呀！请像你在邮件里所说的一样，沉醉于技术、热爱计算机好吗！<br>打这些鸡血并没得卵用，还是习惯更重要啊！</p>

      

      
        
    </div>
  </div>
  
</article>



  
    <article id="post-立个FLAG！" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/22/立个FLAG！/">立个FLAG！</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2018/08/22/立个FLAG！/" class="article-date">
  <time datetime="2018-08-21T22:34:00.000Z" itemprop="datePublished">2018-08-22</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>因为机器学习真的学的不是很好，西瓜书看过的内容都还回去了，看的也是最简单易懂的内容，反复几次也没记住个什么，纯写算法这项工作也没有做，学算法还是不能如此地囫囵吞枣，要学一样东西就彻彻底底地吃透了再move<br>on。</p>
<p>看过五集网易公开课的Andrew<br>Ng机器学习课程，碍于复杂的数学推导以及吴老师极其有个性的板书，我决定还是去看有PPT的coursera吧，半途而废是个很不好的习惯，我也很讨厌自己做事情做到一半或者仅仅开了个头就不做了。这次不能这样子啦！老老实实的把Coursera布置的任务做好，课程之余去搞一些代码自己玩~</p>
<p>一共11周的课程，每日进度我要PO在这里面，监督自己！数学其实是一个很酷的东西，吴老师的课上的我非常激动~性感的数学课~</p>
<p>比较好的参考笔记（我就懒得做电子笔记啦！）：</p>
<ol>
<li><p><a href="https://blog.csdn.net/zhaoxinfan/article/details/8971149" target="_blank" rel="noopener">https://blog.csdn.net/zhaoxinfan/article/details/8971149</a></p>
</li>
<li><p><a href="https://yoyoyohamapi.gitbooks.io/mit-ml/content/线性回归/codes/梯度下降.html" target="_blank" rel="noopener">https://yoyoyohamapi.gitbooks.io/mit-ml/content/线性回归/codes/梯度下降.html</a></p>
</li>
</ol>
<p>希望自己是真实地用脑子记住而且学一部分就可以学以致用，做出一两样be proud of的东西来~</p>

      

      
        
    </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        <aside id="sidebar">
  <nav class="menus">
  	<ul>
  		<li><a href="/"><i class="icon icon-home"></i></a></li>
  		
			<li><a href="/archives"><i class="icon icon-fenlei"></i></a></li>
  		
  		
			<li><a href="/tags"><i class="icon icon-tag"></i></a></li>
  		
  		
  			<li><a href="https://github.com/hejianxian" target="_blank"><i class="icon icon-github"></i></a></li>
  		
  	</ul>
  </nav>
  <a id="go-top" href="#"><i class="icon icon-up"></i></a>
</aside>

      </div>
      <footer id="footer">
  
	<div id="footer-info" class="inner">
	  &copy; 2018 John Doe 
	  - Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	  - Theme <a href="https://github.com/hejianxian/hexo-theme-jane/" target="_blank">Jane</a>
	</div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/tags" class="mobile-nav-link">Tag</a>
  
    <a href="https://github.com/hejianxian" class="mobile-nav-link">Github</a>
  
</nav>
    

<script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>